{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d944fedb-4e2d-40d4-9c7a-c85e4663c649",
   "metadata": {},
   "source": [
    "\n",
    "## The *args and **kwargs are the ones you passed from tuner.search().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319b136-d9b9-4ac7-b8f0-95ecda19db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "        model.compile(\n",
    "            optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32]),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    MyHyperModel(),\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"tune_hypermodel\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d932045-7606-4aa7-9e0e-7fe8aff471e9",
   "metadata": {},
   "source": [
    "\n",
    "## For epochs specifically, I'd alternatively recommend looking at using early stopping during training via passing in the tf.keras.callbacks.EarlyStopping callback if it's applicable to your use case. This can be configured to stop your training as soon as the validation loss stops improving. You can pass Keras callbacks like this to search:\n",
    "\n",
    "# Will stop training if the \"val_loss\" hasn't improved in 3 epochs.\n",
    "tuner.search(x, y, epochs=30, callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=3)])\n",
    "\n",
    "## Ref: https://github.com/keras-team/keras-tuner/issues/122"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85024288-12a0-4019-848e-2b2a456cfdae",
   "metadata": {},
   "source": [
    "\n",
    "## Adding dropout layer to a model architecture - know how!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2cf8a6-c894-48a5-b6f0-0015646b6782",
   "metadata": {},
   "source": [
    "\n",
    "Keras automatically takes care of applying dropout during training and adjusting the layer's output at test time. When you add a tf.keras.layers.Dropout layer to your model, dropout is only active during training. This means that during training, a fraction of the features (as specified by the dropout rate) will be zeroed out at random to prevent overfitting by adding regularization to the network.\n",
    "\n",
    "During testing, validation, or making predictions with model.predict() on new data, Keras automatically disables dropout. Therefore, all neurons will be active, and their outputs will not be zeroed out. However, as mentioned in the context you provided, the layer's output values are scaled down by a factor equal to the dropout rate at training time to compensate for the fact that more units are active than at training time. Keras handles this scaling automatically, so you don't need to adjust the model or specify anything additional when evaluating or using your model for predictions.\n",
    "\n",
    "In summary, when using dropout in Keras:\n",
    "\n",
    "During training: Dropout is applied, and a fraction of the inputs are randomly set to 0.\n",
    "During testing, validation, or prediction: Dropout is not applied, but the outputs are properly scaled to account for the difference in active units between training and test time.\n",
    "You do not need to worry about manually managing dropout when switching between training and testing phases; Keras manages this for you seamlessly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd1667-383a-4aaa-b302-cb089ddda29d",
   "metadata": {},
   "source": [
    "\n",
    "## A LSTM Model\n",
    "Ref: https://www.kaggle.com/code/anseldsouza/nvidia-price-prediction-lstm-using-keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c257115-ddba-4a61-b524-4022d59a35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('input_unit',min_value=32,max_value=512,step=32), return_sequences=True, \n",
    "                   input_shape= ( X_train.shape[1],X_train.shape[2])))\n",
    "    for i in range(hp.Int('n_layers', 1, 4)):\n",
    "        model.add(LSTM(hp.Int(f'lstm_{i}_units',min_value=32,max_value=512,step=32),return_sequences=True))\n",
    "    model.add(LSTM(hp.Int('layer_2_neurons',min_value=32,max_value=512,step=32)))\n",
    "    model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    model.add(Dense(30, activation=hp.Choice('dense_activation',values=['relu', 'sigmoid'],default='relu')))\n",
    "    model.add(Dense(1, activation=hp.Choice('dense_activation',values=['relu', 'sigmoid'],default='relu')))\n",
    "   \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics = ['mse'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "tuner = kt.RandomSearch(model_builder, objective=\"mse\", max_trials = 4, executions_per_trial =2,directory = \"./\")\n",
    "\n",
    "tuner.search(x=X_train, y=y_train, epochs = 10, batch_size =256, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e99aea-130a-4d61-b6f5-d890c240a9b6",
   "metadata": {},
   "source": [
    "## In this example, they have used the best model from the tuning process to predict on the test data \n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "tuner.results_summary()\n",
    "y_pred = best_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3d74c-efbf-445c-9897-b2314b283a52",
   "metadata": {},
   "source": [
    "\n",
    "## Another LSTM Model sample\n",
    "Ref: https://kamran-afzali.github.io/posts/2022-02-20/tuner.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68ed40-f46e-420f-afbf-7f7f8b6b2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('input_unit',min_value=32,max_value=128,step=32),return_sequences=True, input_shape=(1,24)))\n",
    "    for i in range(hp.Int('n_layers', 1, 10)):\n",
    "        model.add(LSTM(hp.Int(f'lstm_{i}_units',min_value=32,max_value=128,step=32),return_sequences=True))\n",
    "    model.add(LSTM(6))\n",
    "    model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    model.add(Dense(6))\n",
    "    model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics = ['mse'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60105b2-16de-4fd9-a1f0-ba80ee4990c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuner= kt.RandomSearch(\n",
    "        build_model,\n",
    "        objective='mse',\n",
    "        max_trials=10,\n",
    "        executions_per_trial=3\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df88b75-eb12-4ca7-adb6-1bfca7bbc3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuner.search(\n",
    "        x=X_train_t,\n",
    "        y=c,\n",
    "        epochs=20,\n",
    "        batch_size=128,\n",
    "        validation_data=(X_test_t,d),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef14825-e03a-4d5a-be8d-6cfaff9faa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5349415-00a4-4f19-bb7d-a240de33e62b",
   "metadata": {},
   "source": [
    "## Logs and checkpoints\n",
    "\n",
    "The my_dir/intro_to_kt directory contains detailed logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional overwrite=True argument while instantiating the tuner.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
